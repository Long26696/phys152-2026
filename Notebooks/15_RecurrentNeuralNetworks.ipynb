{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Follow-up on CNN filters (kernels)\n",
        "\n"
      ],
      "metadata": {
        "id": "52qkzQ6rT0mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "CvnQd4G4Tw6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "wEicEbosT8Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate synthetic sine wave data\n",
        "\n",
        "We'll create a simple sine wave with some noise to simulate realistic time series data."
      ],
      "metadata": {
        "id": "HBwBibEzUQsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sine_wave(n_samples=1000, frequency=0.05,\n",
        "                       amplitude=1.0, noise_std=0.1):\n",
        "    \"\"\"\n",
        "    Generate a sine wave with Gaussian noise.\n",
        "\n",
        "    Args:\n",
        "        n_samples: Number of time steps\n",
        "        frequency: Frequency of sine wave (controls period)\n",
        "        amplitude: Amplitude of sine wave\n",
        "        noise_std: Standard deviation of Gaussian noise\n",
        "\n",
        "    Returns:\n",
        "        t: Time array\n",
        "        y: Noisy sine wave values\n",
        "    \"\"\"\n",
        "    t = np.arange(n_samples)\n",
        "    y = amplitude * np.sin(2 * np.pi * frequency * t)\n",
        "    y += np.random.normal(0, noise_std, n_samples)\n",
        "    return t, y\n",
        "\n",
        "# Generate data\n",
        "t, y = generate_sine_wave(n_samples=1000, frequency=0.05,\n",
        "                          amplitude=1.0, noise_std=0.1)\n",
        "\n",
        "# Plot the time series\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(t[:200], y[:200], label='Noisy sine wave', alpha=0.7)\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Generated Sine Wave Time Series')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Data shape: {y.shape}\")\n",
        "print(f\"Mean: {y.mean():.3f}, Std: {y.std():.3f}\")"
      ],
      "metadata": {
        "id": "Nw7ibb9sUCDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Sequences for Training\n",
        "\n",
        "For RNN training, we need to create input sequences and corresponding targets.\n",
        "- **Input**: Past `seq_length` values: $[y(t-n), ..., y(t-2), y(t-1)]$\n",
        "- **Target**: Next value: $y(t)$"
      ],
      "metadata": {
        "id": "BPNECszzdIAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"\n",
        "    Create sequences for RNN training.\n",
        "\n",
        "    Args:\n",
        "        data: Time series array\n",
        "        seq_length: Length of input sequence\n",
        "\n",
        "    Returns:\n",
        "        X: Input sequences (n_samples, seq_length, 1)\n",
        "        y: Target values (n_samples, 1)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 10  # Use past 10 time steps to predict next value\n",
        "X, y_target = create_sequences(y, seq_length)\n",
        "\n",
        "# Reshape for RNN input (n_samples, seq_length, n_features)\n",
        "X = X.reshape(-1, seq_length, 1)\n",
        "y_target = y_target.reshape(-1, 1)\n",
        "\n",
        "# Split into train and validation sets (80/20 split)\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y_target[:split_idx], y_target[split_idx:]\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_torch = torch.FloatTensor(X_train).to(device)\n",
        "y_train_torch = torch.FloatTensor(y_train).to(device)\n",
        "X_val_torch = torch.FloatTensor(X_val).to(device)\n",
        "y_val_torch = torch.FloatTensor(y_val).to(device)"
      ],
      "metadata": {
        "id": "nXmydEPWUNYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create RNN architecture by hand\n",
        "\n",
        "Let's first implement a very simple RNN cell manually to understand the math:\n",
        "\n",
        "$$h_t = \\tanh(W_x x_t + W_h h_{t-1} + b_h)$$\n",
        "$$y_t = W_y h_t + b_y$$"
      ],
      "metadata": {
        "id": "CInch9SMUah3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNCell:\n",
        "    \"\"\"\n",
        "    Simple RNN cell implementation in NumPy for educational purposes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights with small random values\n",
        "        scale = 0.01\n",
        "        self.W_x = np.random.randn(input_size, hidden_size) * scale  # Input to hidden\n",
        "        self.W_h = np.random.randn(hidden_size, hidden_size) * scale  # Hidden to hidden\n",
        "        self.W_y = np.random.randn(hidden_size, output_size) * scale  # Hidden to output\n",
        "        self.b_h = np.zeros((1, hidden_size))  # Hidden bias\n",
        "        self.b_y = np.zeros((1, output_size))  # Output bias\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass through the RNN.\n",
        "\n",
        "        Args:\n",
        "            X: Input sequence (seq_length, input_size)\n",
        "\n",
        "        Returns:\n",
        "            outputs: Output at each time step\n",
        "            hidden_states: Hidden states at each time step\n",
        "        \"\"\"\n",
        "        seq_length = X.shape[0]\n",
        "        hidden_states = []\n",
        "        outputs = []\n",
        "\n",
        "        # Initialize hidden state to zeros\n",
        "        h = np.zeros((1, self.hidden_size))\n",
        "\n",
        "        # Process sequence one time step at a time\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[t:t+1]  # Shape: (1, input_size)\n",
        "\n",
        "            # Update hidden state: h_t = tanh(W_x * x_t + W_h * h_{t-1} + b_h)\n",
        "            h = np.tanh(np.dot(x_t, self.W_x) + np.dot(h, self.W_h) + self.b_h)\n",
        "\n",
        "            # Compute output: y_t = W_y * h_t + b_y\n",
        "            y = np.dot(h, self.W_y) + self.b_y\n",
        "\n",
        "            hidden_states.append(h.copy())\n",
        "            outputs.append(y.copy())\n",
        "\n",
        "        return np.array(outputs), np.array(hidden_states)\n",
        "\n",
        "# Test the manual implementation\n",
        "manual_rnn = SimpleRNNCell(input_size=1, hidden_size=16, output_size=1)\n",
        "test_input = X_train[0]  # Shape: (10, 1)\n",
        "outputs, hidden_states = manual_rnn.forward(test_input)\n",
        "\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Outputs shape: {outputs.shape}\")\n",
        "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
        "print(f\"\\nFinal output (prediction): {outputs[-1, 0, 0]:.4f}\")\n",
        "print(f\"True target: {y_train[0, 0]:.4f}\")"
      ],
      "metadata": {
        "id": "pGc9uJ-YWa0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Hidden State Evolution\n",
        "\n",
        "Let's see what the hidden state parameters look like before training."
      ],
      "metadata": {
        "id": "Onf6ai48dQ4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hidden states over time\n",
        "hidden_states_reshaped = hidden_states.squeeze()  # Shape: (seq_length, hidden_size)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(test_input, 'o-', label='Input sequence')\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Input Sequence')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(hidden_states_reshaped.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "plt.colorbar(label='Activation')\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('Hidden unit')\n",
        "plt.title('Hidden State Evolution (Untrained)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bIzgE9NUWjsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an UNTRAINED RNN with zero weights, so the hidden states don't encode any meaningful patterns yet.\n",
        "\n",
        "Key observations:\n",
        "1. Hidden state dimension: Each row is one of the 16 hidden units\n",
        "2. Time evolution: Each column is one time step (0-9)\n",
        "3. Values are zero because we haven't trained yet\n",
        "\n",
        "After training, we'll see how the network learns to use these hidden units to encode temporal patterns."
      ],
      "metadata": {
        "id": "DNcxOlPbdkMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and Train RNN with PyTorch\n",
        "\n",
        "Now let's use PyTorch's built-in RNN for more efficient training. We do not have to keep track of the weights ourselves because PyTorch hides those details."
      ],
      "metadata": {
        "id": "Nz6Rhd53d0RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple RNN for time series prediction using PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=1, hidden_size=32, output_size=1, num_layers=1):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        # out shape: (batch_size, seq_length, hidden_size)\n",
        "        # h_n shape: (num_layers, batch_size, hidden_size)\n",
        "        out, h_n = self.rnn(x, h0)\n",
        "\n",
        "        # We only need the output from the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, h_n\n",
        "\n",
        "# Create model\n",
        "model = VanillaRNN(input_size=1, hidden_size=32, output_size=1, num_layers=1).to(device)\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
      ],
      "metadata": {
        "id": "2U-60BYRUOxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the RNN model\n",
        "\n",
        "What are the `criterion` and `optimizer` being used here?\n",
        "\n",
        "How does this training loop differ from the DNN and CNN loops we used in previous sessions?"
      ],
      "metadata": {
        "id": "KLOhwIQGeCWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=100, lr=0.001, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train the RNN model.\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs, _ = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val).item()\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model\n",
        "print(\"Training RNN...\\n\")\n",
        "train_losses, val_losses = train_model(\n",
        "    model, X_train_torch, y_train_torch, X_val_torch, y_val_torch,\n",
        "    epochs=100, lr=0.001, batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "VlvZu5UyUsEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of using the separate validation dataset to make sure we are not overtraining the model on the training dataset.\n",
        "\n",
        "After having seen this plot, what would you change about the training?"
      ],
      "metadata": {
        "id": "cCjIP1xDeZDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', alpha=0.8)\n",
        "plt.plot(val_losses, label='Validation Loss', alpha=0.8)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.6f}\")\n",
        "print(f\"Final Validation Loss: {val_losses[-1]:.6f}\")"
      ],
      "metadata": {
        "id": "7LeKobGmUvK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference and visualization\n",
        "\n",
        "Predict the entire time series given the range of time values (X values)."
      ],
      "metadata": {
        "id": "EXXJ_PF5eqnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_pred, _ = model(X_train_torch)\n",
        "    val_pred, _ = model(X_val_torch)\n",
        "\n",
        "# Convert to numpy for plotting\n",
        "train_pred = train_pred.cpu().numpy()\n",
        "val_pred = val_pred.cpu().numpy()\n",
        "\n",
        "# Plot predictions vs ground truth\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Training set predictions\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(y_train[:200], label='True', alpha=0.7)\n",
        "plt.plot(train_pred[:200], label='Predicted', alpha=0.7)\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Training Set: Predictions vs Ground Truth')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation set predictions\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(y_val, label='True', alpha=0.7)\n",
        "plt.plot(val_pred, label='Predicted', alpha=0.7)\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Validation Set: Predictions vs Ground Truth')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics\n",
        "train_mse = np.mean((y_train - train_pred)**2)\n",
        "val_mse = np.mean((y_val - val_pred)**2)\n",
        "\n",
        "print(f\"\\nTraining Set - MSE: {train_mse:.6f}\")\n",
        "print(f\"Validation Set - MSE: {val_mse:.6f}\")"
      ],
      "metadata": {
        "id": "idpGvZJkUySs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the hidden states\n",
        "\n",
        "What have the RNN's hidden states learned from the data?\n",
        "\n",
        "Remember that they were all zeroes before training. After training, we see the weights have adapted to the features in the periodic time series data. They have learned what to remember."
      ],
      "metadata": {
        "id": "Si1KmN7QfhXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_states(model, X):\n",
        "    \"\"\"\n",
        "    Extract hidden states from RNN for given input.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get all hidden states at each time step\n",
        "        h0 = torch.zeros(model.num_layers, X.size(0), model.hidden_size).to(X.device)\n",
        "        rnn_out, _ = model.rnn(X, h0)\n",
        "    return rnn_out\n",
        "\n",
        "# Get hidden states for a few validation sequences\n",
        "sample_indices = [0, 50, 100]\n",
        "\n",
        "fig, axes = plt.subplots(len(sample_indices), 2, figsize=(14, 4*len(sample_indices)))\n",
        "\n",
        "for idx, sample_idx in enumerate(sample_indices):\n",
        "    # Get single sequence\n",
        "    X_sample = X_val_torch[sample_idx:sample_idx+1]  # Shape: (1, seq_length, 1)\n",
        "    y_true = y_val[sample_idx, 0]\n",
        "\n",
        "    # Get hidden states\n",
        "    hidden_states = get_hidden_states(model, X_sample)\n",
        "    hidden_states = hidden_states.cpu().numpy().squeeze()  # Shape: (seq_length, hidden_size)\n",
        "\n",
        "    # Get prediction\n",
        "    y_pred, _ = model(X_sample)\n",
        "    y_pred = y_pred.cpu().detach().numpy()[0, 0]\n",
        "\n",
        "    # Plot input sequence\n",
        "    axes[idx, 0].plot(X_sample.cpu().numpy().squeeze(), 'o-', label='Input sequence')\n",
        "    axes[idx, 0].axhline(y=y_true, color='g', linestyle='--', label=f'True next: {y_true:.3f}')\n",
        "    axes[idx, 0].axhline(y=y_pred, color='r', linestyle='--', label=f'Predicted: {y_pred:.3f}')\n",
        "    axes[idx, 0].set_xlabel('Time step')\n",
        "    axes[idx, 0].set_ylabel('Value')\n",
        "    axes[idx, 0].set_title(f'Sample {sample_idx}: Input Sequence')\n",
        "    axes[idx, 0].legend()\n",
        "    axes[idx, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot hidden states\n",
        "    im = axes[idx, 1].imshow(hidden_states.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "    axes[idx, 1].set_xlabel('Time step')\n",
        "    axes[idx, 1].set_ylabel('Hidden unit')\n",
        "    axes[idx, 1].set_title(f'Sample {sample_idx}: Hidden State Evolution')\n",
        "    plt.colorbar(im, ax=axes[idx, 1], label='Activation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4pp8cYAacvd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In particular, we can see\n",
        "- Different hidden units activate at different times. Some units respond to rising trends, and others to falling trends. The pattern of activations encodes the sequence history\n",
        "- No single unit stores *the past*. It is distributed across all units.\n",
        "- The final hidden state (rightmost column) contains compressed info that is used by the output layer to predict the next value.\n",
        "- Rising sine waves may activate different units than falling ones. This is the *memory* mechanism in RNNs.\n",
        "\n",
        "In summary, the hidden states compress the past into a fixed-size representation for future prediction."
      ],
      "metadata": {
        "id": "-ilWrFs8XC7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning\n",
        "import torch, torch.nn as nn, torch.utils.data as data, torchvision as tv, torch.nn.functional as F\n",
        "import lightning as L"
      ],
      "metadata": {
        "id": "JAnAGQaJ2e8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing Gradients motivate Long Short-Term Networks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3IKh8kLagppS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "GNo255p5gpEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Delayed Echo Data\n",
        "\n",
        "This is a synthetic task designed to test long-term memory:\n",
        "\n",
        "**Input:** A sequence with a signal (value 1) at an early position, rest are zeros\n",
        "```\n",
        "[0, 0, 1, 0, 0, 0, ..., 0, 0, 0] (signal at position 2)\n",
        "```\n",
        "\n",
        "**Target:** Echo the signal position after a delay\n",
        "```\n",
        "[0, 0, 0, 0, ..., 0, 1, 0, 0, 0]  (echo appears after delay)\n",
        "```\n",
        "\n",
        "The network needs to remember the signal position even after many time steps.\n",
        "\n",
        "First, we need to create the data."
      ],
      "metadata": {
        "id": "_Pxx6wvzpAo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_delayed_echo_data(n_samples=1000, seq_length=50, delay=20, signal_position_range=(5, 10)):\n",
        "    \"\"\"\n",
        "    Create delayed echo task dataset.\n",
        "\n",
        "    Args:\n",
        "        n_samples: Number of sequences to generate\n",
        "        seq_length: Total length of each sequence\n",
        "        delay: Number of steps between signal and echo\n",
        "        signal_position_range: Range where signal can appear (start, end)\n",
        "\n",
        "    Returns:\n",
        "        X: Input sequences (n_samples, seq_length, 1)\n",
        "        y: Target sequences (n_samples, seq_length, 1)\n",
        "    \"\"\"\n",
        "    X = np.zeros((n_samples, seq_length, 1))\n",
        "    y = np.zeros((n_samples, seq_length, 1))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Randomly place signal\n",
        "        signal_pos = np.random.randint(signal_position_range[0], signal_position_range[1])\n",
        "        X[i, signal_pos, 0] = 1.0\n",
        "\n",
        "        # Place echo after delay\n",
        "        echo_pos = signal_pos + delay\n",
        "        if echo_pos < seq_length:\n",
        "            y[i, echo_pos, 0] = 1.0\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Create datasets with different delay lengths\n",
        "delays = [20, 40, 60]\n",
        "datasets = {}\n",
        "\n",
        "for delay in delays:\n",
        "    X, y = create_delayed_echo_data(n_samples=1000, seq_length=80, delay=delay)\n",
        "\n",
        "    # Split into train/val\n",
        "    split = int(0.8 * len(X))\n",
        "    datasets[delay] = {\n",
        "        'X_train': torch.FloatTensor(X[:split]).to(device),\n",
        "        'y_train': torch.FloatTensor(y[:split]).to(device),\n",
        "        'X_val': torch.FloatTensor(X[split:]).to(device),\n",
        "        'y_val': torch.FloatTensor(y[split:]).to(device)\n",
        "    }\n",
        "\n",
        "print(f\"Created datasets for delays: {delays}\")\n",
        "print(f\"Sequence length: 50\")\n",
        "print(f\"Training samples per delay: 800\")\n",
        "print(f\"Validation samples per delay: 200\")"
      ],
      "metadata": {
        "id": "i4mjul2ziY4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the data"
      ],
      "metadata": {
        "id": "6yGvAB7ZpgKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show examples for each delay\n",
        "fig, axes = plt.subplots(len(delays), 2, figsize=(14, 3*len(delays)))\n",
        "\n",
        "for idx, delay in enumerate(delays):\n",
        "    X_example = datasets[delay]['X_train'][0].cpu().numpy().squeeze()\n",
        "    y_example = datasets[delay]['y_train'][0].cpu().numpy().squeeze()\n",
        "\n",
        "    # Plot input\n",
        "    axes[idx, 0].stem(X_example, basefmt=' ')\n",
        "    axes[idx, 0].set_ylim(-0.1, 1.5)\n",
        "    axes[idx, 0].set_xlabel('Time step')\n",
        "    axes[idx, 0].set_ylabel('Value')\n",
        "    axes[idx, 0].set_title(f'Input (Delay={delay}): Signal appears early')\n",
        "    axes[idx, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot target\n",
        "    axes[idx, 1].stem(y_example, basefmt=' ', linefmt='C1-', markerfmt='C1o')\n",
        "    axes[idx, 1].set_ylim(-0.1, 1.5)\n",
        "    axes[idx, 1].set_xlabel('Time step')\n",
        "    axes[idx, 1].set_ylabel('Value')\n",
        "    axes[idx, 1].set_title(f'Target (Delay={delay}): Echo appears after delay')\n",
        "    axes[idx, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k9mmGbe9ibQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We want to train a network to remember where the signal appeared and echo it after the delay.\n",
        "\n",
        "Obviously this will require maintaining information across many time steps."
      ],
      "metadata": {
        "id": "P8fZVfkHiiY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the vanilla RNN using simple architecture"
      ],
      "metadata": {
        "id": "sTVUmwA1pm8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, output_size=1, num_layers=1):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, h_n = self.rnn(x, h0)\n",
        "        # Output at all time steps (not just last)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def train_echo_model(model, X_train, y_train, X_val, y_val, epochs=50, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train model on delayed echo task.\n",
        "    \"\"\"\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Binary classification at each time step\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            # Calculate accuracy (correct echo detection)\n",
        "            predictions = (torch.sigmoid(val_outputs) > 0.5).float()\n",
        "            accuracy = (predictions == y_val).float().mean().item()\n",
        "            val_accuracies.append(accuracy)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, \"\n",
        "                  f\"Val Loss: {val_loss.item():.4f}, Val Acc: {accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "# Train vanilla RNN on all delays\n",
        "vanilla_results = {}\n",
        "\n",
        "for delay in delays:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training Vanilla RNN with delay={delay}\")\n",
        "    print('='*60)\n",
        "\n",
        "    model = VanillaRNN(input_size=1, hidden_size=64, output_size=1).to(device)\n",
        "\n",
        "    train_losses, val_losses, val_accs = train_echo_model(\n",
        "        model,\n",
        "        datasets[delay]['X_train'], datasets[delay]['y_train'],\n",
        "        datasets[delay]['X_val'], datasets[delay]['y_val'],\n",
        "        epochs=50, lr=0.001\n",
        "    )\n",
        "\n",
        "    vanilla_results[delay] = {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accuracies': val_accs,\n",
        "        'final_accuracy': val_accs[-1]\n",
        "    }"
      ],
      "metadata": {
        "id": "yxxOS8ySidVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the vanilla RNN performance\n",
        "\n"
      ],
      "metadata": {
        "id": "ZubWjgVIptqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "for delay in delays:\n",
        "    axes[0].plot(vanilla_results[delay]['val_losses'], label=f'Delay={delay}', alpha=0.7)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Validation Loss')\n",
        "axes[0].set_title('Vanilla RNN: Loss vs Delay Length')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "for delay in delays:\n",
        "    axes[1].plot(vanilla_results[delay]['val_accuracies'], label=f'Delay={delay}', alpha=0.7)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Validation Accuracy')\n",
        "axes[1].set_title('Vanilla RNN: Accuracy vs Delay Length')\n",
        "axes[1].axhline(y=0.98, color='gray', linestyle='--', alpha=0.5, label='98% threshold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFinal Validation Accuracies (Vanilla RNN):\")\n",
        "for delay in delays:\n",
        "    acc = vanilla_results[delay]['final_accuracy']\n",
        "    print(f\"  Delay={delay}: {acc:.4f} ({acc*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "4ne-bPaylv4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might have expected the RNN to do worse as the delay increases, but this is actually pretty good!"
      ],
      "metadata": {
        "id": "JOXcAaMpqO6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_norms(model, X, y, criterion):\n",
        "    \"\"\"\n",
        "    Compute gradient norms at different time steps.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect gradient norms\n",
        "    grad_norms = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norms.append(param.grad.norm().item())\n",
        "\n",
        "    return grad_norms\n",
        "\n",
        "# Compute gradients for different delays\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for delay in delays:\n",
        "    model = vanilla_results[delay]['model']\n",
        "    X_sample = datasets[delay]['X_val'][:10]\n",
        "    y_sample = datasets[delay]['y_val'][:10]\n",
        "\n",
        "    grad_norms = compute_gradient_norms(model, X_sample, y_sample, criterion)\n",
        "    plt.plot(grad_norms, 'o-', label=f'Delay={delay}', alpha=0.7)\n",
        "\n",
        "plt.xlabel('Parameter Index')\n",
        "plt.ylabel('Gradient Norm')\n",
        "plt.title('Vanilla RNN: Gradient Magnitudes by Delay')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bYUqZlwHlz7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "\n",
        "We expected there to be some vanishing gradient as a function of delay, but these gradients do not seem to be vanishing.\n",
        "\n",
        "Nevertheless we'll implement the LSTM as an alternative model for this application.\n",
        "\n",
        "Two questions:\n",
        "- How is the LSTM architecture definition different from the RNN definition?\n",
        "- How is the LSTM training loop different from the RNN training loop?"
      ],
      "metadata": {
        "id": "jXdU49Dtqg0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, output_size=1, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Train LSTM on all delays\n",
        "lstm_results = {}\n",
        "\n",
        "for delay in delays:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training LSTM with delay={delay}\")\n",
        "    print('='*60)\n",
        "\n",
        "    model = LSTMModel(input_size=1, hidden_size=64, output_size=1).to(device)\n",
        "\n",
        "    train_losses, val_losses, val_accs = train_echo_model(\n",
        "        model,\n",
        "        datasets[delay]['X_train'], datasets[delay]['y_train'],\n",
        "        datasets[delay]['X_val'], datasets[delay]['y_val'],\n",
        "        epochs=50, lr=0.001\n",
        "    )\n",
        "\n",
        "    lstm_results[delay] = {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accuracies': val_accs,\n",
        "        'final_accuracy': val_accs[-1]\n",
        "    }"
      ],
      "metadata": {
        "id": "BteXKdrKl2jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the performance of the LSTM"
      ],
      "metadata": {
        "id": "lPg-wMLcrcDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "for delay in delays:\n",
        "    axes[0].plot(vanilla_results[delay]['val_accuracies'],\n",
        "                 label=f'RNN (delay={delay})', alpha=0.5, linestyle='--')\n",
        "    axes[0].plot(lstm_results[delay]['val_accuracies'],\n",
        "                 label=f'LSTM (delay={delay})', alpha=0.8)\n",
        "\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Validation Accuracy')\n",
        "axes[0].set_title('Accuracy: LSTM vs Vanilla RNN')\n",
        "axes[0].axhline(y=0.98, color='gray', linestyle=':', alpha=0.5)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Bar chart of final accuracies\n",
        "x_pos = np.arange(len(delays))\n",
        "width = 0.35\n",
        "\n",
        "vanilla_accs = [vanilla_results[d]['final_accuracy'] for d in delays]\n",
        "lstm_accs = [lstm_results[d]['final_accuracy'] for d in delays]\n",
        "\n",
        "axes[1].bar(x_pos - width/2, vanilla_accs, width, label='Vanilla RNN', alpha=0.7)\n",
        "axes[1].bar(x_pos + width/2, lstm_accs, width, label='LSTM', alpha=0.7)\n",
        "\n",
        "axes[1].set_xlabel('Delay Length')\n",
        "axes[1].set_ylabel('Final Validation Accuracy')\n",
        "axes[1].set_title('Final Performance: LSTM vs Vanilla RNN')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(delays)\n",
        "axes[1].axhline(y=0.98, color='gray', linestyle=':', alpha=0.5)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n{'Delay':<10} {'Vanilla RNN':<15} {'LSTM':<15} {'Improvement'}\")\n",
        "print(\"-\"*60)\n",
        "for delay in delays:\n",
        "    v_acc = vanilla_results[delay]['final_accuracy']\n",
        "    l_acc = lstm_results[delay]['final_accuracy']\n",
        "    improvement = ((l_acc - v_acc) / v_acc * 100) if v_acc > 0 else float('inf')\n",
        "    print(f\"{delay:<10} {v_acc:<15.4f} {l_acc:<15.4f} {improvement:>+.1f}%\")\n"
      ],
      "metadata": {
        "id": "tlMZ8Ct-l5pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axxpAIOAl-fN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}